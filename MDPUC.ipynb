{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP with Unmeasured Confounders\n",
    "\n",
    "The model we are using:  \n",
    "Medical Treatment model as defined in Appendix A in https://causalai.net/mdp-causal.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physicains_policy(St, Mt, Et):\n",
    "    return (St+Mt+Et) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# probability_yt_is_1 is a 4 dimensional array\n",
    "# format: probability_yt_is_1[St][Mt][Et][Xt] is P(Yt = 1 | Xt, St, Mt, Et)\n",
    "probability_yt_is_1 = np.zeros((2,2,2,2))\n",
    "probability_yt_is_1[0][0][0][0] = 0.2\n",
    "probability_yt_is_1[0][0][0][1] = 0.9\n",
    "probability_yt_is_1[0][0][1][0] = 0.9\n",
    "probability_yt_is_1[0][0][1][1] = 0.2\n",
    "probability_yt_is_1[0][1][0][0] = 0.8\n",
    "probability_yt_is_1[0][1][0][1] = 0.3\n",
    "probability_yt_is_1[0][1][1][0] = 0.3\n",
    "probability_yt_is_1[0][1][1][1] = 0.8\n",
    "\n",
    "probability_yt_is_1[1][0][0][0] = 0.7\n",
    "probability_yt_is_1[1][0][0][1] = 0.2\n",
    "probability_yt_is_1[1][0][1][0] = 0.2\n",
    "probability_yt_is_1[1][0][1][1] = 0.7\n",
    "probability_yt_is_1[1][1][0][0] = 0.1\n",
    "probability_yt_is_1[1][1][0][1] = 0.8\n",
    "probability_yt_is_1[1][1][1][0] = 0.8\n",
    "probability_yt_is_1[1][1][1][1] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_prob is a 2 dimensional array\n",
    "# format: transition_prob[Xt][St] = P(St+1 = 0 | St, Xt)\n",
    "transition_prob = np.zeros((2,2))\n",
    "transition_prob[0][0] = 0.9\n",
    "transition_prob[0][1] = 0.3\n",
    "transition_prob[1][0] = 0.7\n",
    "transition_prob[1][1] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Yt(St, Mt, Et, Xt):\n",
    "    u = np.random.rand()\n",
    "    if u < probability_yt_is_1[St][Mt][Et][Xt]:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(St, Xt):\n",
    "    u = np.random.rand()\n",
    "    if u < transition_prob[Xt][St]:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_trajectory(patient_id):\n",
    "    global data_df\n",
    "    St = np.random.randint(2)\n",
    "    for t in range(100):\n",
    "        Mt, Et = np.random.randint(2), np.random.randint(2)\n",
    "        Xt = physicains_policy(St, Mt, Et)\n",
    "        Yt = get_Yt(St, Mt, Et, Xt)\n",
    "        data_df = data_df.append({'pt_id': patient_id,'t': t, 'St': St, 'Mt': Mt, 'Et': Et, 'Xt': Xt, 'Yt': Yt}, ignore_index=True)\n",
    "        St = get_next_state(St, Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in range(18242):\n",
    "    if patient_id % 1000 == 0:\n",
    "        print(\"Iteration number: \", patient_id)\n",
    "    generate_single_trajectory(patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('/Users/faaiz/MDPUC/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Et</th>\n",
       "      <th>Mt</th>\n",
       "      <th>St</th>\n",
       "      <th>Xt</th>\n",
       "      <th>Yt</th>\n",
       "      <th>pt_id</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Et   Mt   St   Xt   Yt  pt_id    t\n",
       "0  1.0  0.0  0.0  1.0  0.0    0.0  0.0\n",
       "1  0.0  1.0  0.0  1.0  0.0    0.0  1.0\n",
       "2  0.0  1.0  0.0  1.0  1.0    0.0  2.0\n",
       "3  1.0  0.0  0.0  1.0  1.0    0.0  3.0\n",
       "4  1.0  0.0  0.0  1.0  0.0    0.0  4.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Define parameters #################\n",
    "\n",
    "n_states = 2\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = data_df['pt_id'].unique()\n",
    "training = patients[np.random.randint(5, size = (len(patients))) != 4]\n",
    "testing = patients[np.random.randint(5, size = (len(patients))) == 4]\n",
    "\n",
    "train_data = data_df.loc[data_df['pt_id'].isin(training)].reset_index()\n",
    "test_data = data_df.loc[data_df['pt_id'].isin(testing)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding optimal policy assuming the confounders $E_{t}$ and $M_{t}$ are not measured\n",
    "The states of MDP are $S_t$, actions are $X_t$ and rewards are $Y_t$\n",
    "### 1. Estimating the Transition matrix T(S,S',A) and Reward matrix R(S,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = np.zeros((n_states,n_states,n_actions))\n",
    "sums = np.zeros((n_states,n_actions))\n",
    "R = np.zeros((n_states,n_actions))\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    if index < len(train_data)-1 and train_data.at[index + 1, 't'] != 0:\n",
    "        S0, S1, action, Y = int(train_data.at[index, 'St']), int(train_data.at[index+1, 'St']), int(train_data.at[index, 'Xt']), int(train_data.at[index, 'Yt'])\n",
    "        transition[S0][S1][action] += 1\n",
    "        R[S0][action] += Y\n",
    "        sums[S0][action] += 1\n",
    "        \n",
    "for i in range(n_states):\n",
    "    for j in range(n_actions):\n",
    "        if sums[i][j] != 0:\n",
    "            R[i][j] = R[i][j]/sums[i][j]\n",
    "            for k in range(n_states):\n",
    "                transition[i][k][j] = transition[i][k][j]/sums[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check to make sure transition matrix estimate is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.89987782, 0.70066325],\n",
       "        [0.10012218, 0.29933675]],\n",
       "\n",
       "       [[0.29994861, 0.80034186],\n",
       "        [0.70005139, 0.19965814]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate physician's policy from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "physpol = (sums.T/((sums.sum(axis=1)==0) + (sums.sum(axis=1)))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MDP policy iteration util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_computePpolicyPRpolicy(P, R, policy):\n",
    "    Ppolicy = np.zeros((n_states,n_states))\n",
    "    PRpolicy = np.zeros(n_states)\n",
    "    for a in range(n_actions):\n",
    "        ind = policy==a\n",
    "        if ind.sum()>0:\n",
    "            for j in range(n_states):\n",
    "                if ind[j]:\n",
    "                    for i in range(n_states):\n",
    "                        Ppolicy[j][i] = P[j][i][a]\n",
    "                    PRpolicy[j] = R[j][a]\n",
    "    return Ppolicy, PRpolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_eval_policy_matrix(P,R,discount,policy):\n",
    "    Ppolicy, PRpolicy = mdp_computePpolicyPRpolicy(P, R, policy)\n",
    "    Vpolicy = np.matmul(np.linalg.inv(np.identity(n_states) - discount*Ppolicy), PRpolicy)\n",
    "    return Vpolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_bellman_operator_with_Q(P, PR, discount, Vprev):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for a in range(n_actions):\n",
    "        for i in range(n_states):\n",
    "            next_step_reward = 0\n",
    "            for j in range(n_states):\n",
    "                next_step_reward += P[i][j][a]*Vprev[j]\n",
    "            Q[i][a] = PR[i][a] + discount*next_step_reward\n",
    "    V = Q.max(axis = 1)\n",
    "    policy = Q.argmax(axis = 1)\n",
    "    return V, Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_policy_iteration_with_Q(P, R, discount, policy0, max_iter=1000):\n",
    "    i = 0\n",
    "    policy = policy0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        i += 1\n",
    "        V = mdp_eval_policy_matrix(P,R,discount,policy)\n",
    "        nil, Q, policy_next = mdp_bellman_operator_with_Q(P,R,discount,V)\n",
    "        n_different = (policy_next != policy).sum()\n",
    "        if n_different == 0 or i == max_iter or (i > 20 and n_different <= 5):\n",
    "            is_done = True\n",
    "        else:\n",
    "            policy = policy_next\n",
    "    return V, policy, i, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Do policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "_, _, i, Qon = mdp_policy_iteration_with_Q(transition, R, gamma, np.ones(n_states))\n",
    "OptimalAction = Qon.argmax(axis=1) # deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.8969691 , 23.87507313],\n",
       "       [23.73126535, 23.78559731]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Off policy evaluation on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.01\n",
    "softpi = physpol.copy()\n",
    "for i in range(n_states):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
